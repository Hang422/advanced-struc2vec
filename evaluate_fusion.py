#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯„ä¼°ç‰¹å¾èåˆæ–¹æ³•
"""
import os
import sys
import time
import pickle
import numpy as np
import networkx as nx
from sklearn.linear_model import LogisticRegression

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from GraphEmbedding.ge.classify import read_node_label, Classifier
from algorithms.traditional.struc2vec import Struc2Vec
from algorithms.graphlet_based.compute_edges_improved import generate_improved_structural_distance

def generate_original_structural_distance(graph_path, output_path):
    """ç”ŸæˆåŸå§‹ struc2vec çš„ç»“æ„è·ç¦»æ–‡ä»¶"""
    print("ç”ŸæˆåŸå§‹ç»“æ„è·ç¦»...")
    
    # åˆ›å»ºä¸´æ—¶çš„ struc2vec å¯¹è±¡æ¥ç”Ÿæˆè·ç¦»
    G = nx.read_edgelist(graph_path, nodetype=str, create_using=nx.DiGraph())
    model = Struc2Vec(G, walk_length=10, num_walks=1, workers=1, verbose=0,
                     opt1_reduce_len=True, opt2_reduce_sim_calc=True)
    
    # è·ç¦»æ–‡ä»¶ä¼šåœ¨ä¸´æ—¶ç›®å½•ä¸­ç”Ÿæˆ
    temp_dist_file = os.path.join(model.temp_path, "structural_dist.pkl")
    
    # å¤åˆ¶åˆ°è¾“å‡ºè·¯å¾„
    import shutil
    shutil.copy(temp_dist_file, output_path)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    shutil.rmtree(model.temp_path)
    
    print(f"âœ… åŸå§‹è·ç¦»æ–‡ä»¶ç”Ÿæˆ: {output_path}")

def fuse_distances(dist1_path, dist2_path, output_path, method='weighted', alpha=0.5):
    """èåˆä¸¤ä¸ªè·ç¦»æ–‡ä»¶"""
    with open(dist1_path, 'rb') as f:
        dist1 = pickle.load(f)
    with open(dist2_path, 'rb') as f:
        dist2 = pickle.load(f)
    
    fused = {}
    
    if method == 'weighted':
        # åŠ æƒå¹³å‡èåˆ
        for pair in set(dist1.keys()).intersection(dist2.keys()):
            fused[pair] = {}
            layers1 = dist1[pair]
            layers2 = dist2[pair]
            for layer in set(layers1.keys()).intersection(layers2.keys()):
                fused[pair][layer] = alpha * layers1[layer] + (1 - alpha) * layers2[layer]
                
    elif method == 'min':
        # å–æœ€å°å€¼èåˆ
        for pair in set(dist1.keys()).intersection(dist2.keys()):
            fused[pair] = {}
            layers1 = dist1[pair]
            layers2 = dist2[pair]
            for layer in set(layers1.keys()).intersection(layers2.keys()):
                fused[pair][layer] = min(layers1[layer], layers2[layer])
                
    elif method == 'max':
        # å–æœ€å¤§å€¼èåˆ
        for pair in set(dist1.keys()).intersection(dist2.keys()):
            fused[pair] = {}
            layers1 = dist1[pair]
            layers2 = dist2[pair]
            for layer in set(layers1.keys()).intersection(layers2.keys()):
                fused[pair][layer] = max(layers1[layer], layers2[layer])
                
    elif method == 'adaptive':
        # è‡ªé€‚åº”èåˆï¼ˆæ ¹æ®å±‚æ¬¡è°ƒæ•´æƒé‡ï¼‰
        for pair in set(dist1.keys()).intersection(dist2.keys()):
            fused[pair] = {}
            layers1 = dist1[pair]
            layers2 = dist2[pair]
            for layer in set(layers1.keys()).intersection(layers2.keys()):
                # ä½å±‚æ›´ä¿¡ä»»åº¦åºåˆ—ï¼Œé«˜å±‚æ›´ä¿¡ä»» graphlet
                layer_alpha = min(0.8, 0.3 + 0.1 * layer)
                fused[pair][layer] = layer_alpha * layers2[layer] + (1 - layer_alpha) * layers1[layer]
    
    with open(output_path, 'wb') as f:
        pickle.dump(fused, f)
    
    return fused

def evaluate_fusion_methods():
    """è¯„ä¼°ä¸åŒçš„èåˆæ–¹æ³•"""
    print("=" * 80)
    print("ç‰¹å¾èåˆæ–¹æ³•è¯„ä¼°")
    print("=" * 80)
    
    # è®¾ç½®è·¯å¾„
    base_dir = os.path.dirname(os.path.abspath(__file__))
    graph_path = os.path.join(base_dir, "data/flight/brazil-airports.edgelist")
    label_path = os.path.join(base_dir, "data/flight/labels-brazil-airports.txt")
    output_dir = os.path.join(base_dir, "output")
    
    # è·ç¦»æ–‡ä»¶è·¯å¾„
    original_dist = os.path.join(output_dir, "structural_dist_original.pkl")
    graphlet_dist = os.path.join(output_dir, "structural_dist_brazil-airports.pkl")
    
    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æ•°æ®...")
    G = nx.read_edgelist(graph_path, nodetype=str, create_using=nx.DiGraph())
    X, Y = read_node_label(label_path, skip_head=True)
    
    print(f"å›¾ä¿¡æ¯: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")
    print(f"æ ‡ç­¾ä¿¡æ¯: {len(X)} ä¸ªæ ‡è®°èŠ‚ç‚¹")
    
    # ç”ŸæˆåŸå§‹è·ç¦»æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists(original_dist):
        generate_original_structural_distance(graph_path, original_dist)
    
    # ç¡®ä¿ graphlet è·ç¦»æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(graphlet_dist):
        print("\nç”Ÿæˆ Graphlet è·ç¦»æ–‡ä»¶...")
        generate_improved_structural_distance(
            graph_path,
            graphlet_dist,
            max_layer=3,
            distance_method='frobenius',
            use_orbit_selection=False
        )
    
    results = {}
    
    # 1. åŸºå‡†æ–¹æ³•
    print("\n1. è¯„ä¼°åŸºå‡†æ–¹æ³•...")
    
    # åŸå§‹ struc2vec
    print("   - åŸå§‹ Struc2Vec...")
    start = time.time()
    model = Struc2Vec(G, walk_length=40, num_walks=10, workers=2, verbose=0,
                     structural_dist_file=original_dist)
    model.train(embed_size=64, window_size=5, workers=2, iter=3)
    embeddings = model.get_embeddings()
    clf = Classifier(embeddings=embeddings, clf=LogisticRegression(max_iter=1000))
    metrics = clf.split_train_evaluate(X, Y, 0.8)
    results['Original'] = {
        'time': time.time() - start,
        'metrics': metrics
    }
    
    # Graphlet å¢å¼ºç‰ˆ
    print("   - Graphlet å¢å¼ºç‰ˆ...")
    start = time.time()
    model = Struc2Vec(G, walk_length=40, num_walks=10, workers=2, verbose=0,
                     structural_dist_file=graphlet_dist)
    model.train(embed_size=64, window_size=5, workers=2, iter=3)
    embeddings = model.get_embeddings()
    clf = Classifier(embeddings=embeddings, clf=LogisticRegression(max_iter=1000))
    metrics = clf.split_train_evaluate(X, Y, 0.8)
    results['Graphlet'] = {
        'time': time.time() - start,
        'metrics': metrics
    }
    
    # 2. èåˆæ–¹æ³•
    fusion_methods = {
        'Weighted (Î±=0.5)': {'method': 'weighted', 'alpha': 0.5},
        'Weighted (Î±=0.3)': {'method': 'weighted', 'alpha': 0.3},
        'Weighted (Î±=0.7)': {'method': 'weighted', 'alpha': 0.7},
        'Min Fusion': {'method': 'min'},
        'Max Fusion': {'method': 'max'},
        'Adaptive': {'method': 'adaptive'}
    }
    
    print("\n2. è¯„ä¼°èåˆæ–¹æ³•...")
    for name, params in fusion_methods.items():
        print(f"   - {name}...")
        
        # ç”Ÿæˆèåˆè·ç¦»
        fused_path = os.path.join(output_dir, f"structural_dist_fused_{params['method']}.pkl")
        fuse_distances(
            original_dist, 
            graphlet_dist, 
            fused_path,
            method=params['method'],
            alpha=params.get('alpha', 0.5)
        )
        
        # è®­ç»ƒå’Œè¯„ä¼°
        start = time.time()
        model = Struc2Vec(G, walk_length=40, num_walks=10, workers=2, verbose=0,
                         structural_dist_file=fused_path)
        model.train(embed_size=64, window_size=5, workers=2, iter=3)
        embeddings = model.get_embeddings()
        clf = Classifier(embeddings=embeddings, clf=LogisticRegression(max_iter=1000))
        metrics = clf.split_train_evaluate(X, Y, 0.8)
        results[name] = {
            'time': time.time() - start,
            'metrics': metrics
        }
    
    # 3. æ‰“å°ç»“æœæ¯”è¾ƒ
    print("\n" + "=" * 80)
    print("ç»“æœæ±‡æ€»")
    print("=" * 80)
    
    print(f"\n{'æ–¹æ³•':<20} {'å‡†ç¡®ç‡':<10} {'F1 Micro':<10} {'F1 Macro':<10} {'è®­ç»ƒæ—¶é—´':<10}")
    print("-" * 70)
    
    for method, data in results.items():
        metrics = data['metrics']
        print(f"{method:<20} {metrics['acc']:<10.4f} {metrics['micro']:<10.4f} "
              f"{metrics['macro']:<10.4f} {data['time']:<10.2f}")
    
    # 4. æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    best_method = max(results.keys(), key=lambda k: results[k]['metrics']['acc'])
    best_acc = results[best_method]['metrics']['acc']
    
    print(f"\næœ€ä½³æ–¹æ³•: {best_method} (å‡†ç¡®ç‡: {best_acc:.4f})")
    
    # 5. æ”¹è¿›åˆ†æ
    print("\næ”¹è¿›åˆ†æ:")
    original_acc = results['Original']['metrics']['acc']
    for method, data in results.items():
        if method != 'Original':
            improvement = (data['metrics']['acc'] - original_acc) / original_acc * 100
            print(f"  {method}: {improvement:+.1f}%")
    
    return results

if __name__ == "__main__":
    try:
        results = evaluate_fusion_methods()
        print("\nâœ… èåˆè¯„ä¼°å®Œæˆ!")
        
        # ç»™å‡ºå»ºè®®
        print("\nğŸ’¡ å»ºè®®:")
        print("  1. å¦‚æœåŸå§‹æ–¹æ³•å·²ç»å¾ˆå¥½ï¼Œå¯ä»¥å°è¯•è¾ƒå°çš„ Î± å€¼ (0.2-0.3)")
        print("  2. è‡ªé€‚åº”èåˆå¯èƒ½åœ¨å¤§å›¾ä¸Šæ•ˆæœæ›´å¥½")
        print("  3. å¯ä»¥å°è¯•å…¶ä»–èåˆç­–ç•¥ï¼Œå¦‚åŸºäºèŠ‚ç‚¹ç‰¹æ€§çš„åŠ¨æ€æƒé‡")
        
    except Exception as e:
        print(f"\nâŒ å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()