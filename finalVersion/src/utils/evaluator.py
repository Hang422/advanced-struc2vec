#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯„ä¼°å·¥å…·
"""
import sys
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# æ·»åŠ çˆ¶é¡¹ç›®è·¯å¾„
parent_path = Path(__file__).parent.parent.parent.parent
sys.path.append(str(parent_path))

from GraphEmbedding.ge.classify import Classifier

class Evaluator:
    """ç®—æ³•è¯„ä¼°å™¨"""
    
    def __init__(self, **kwargs):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            **kwargs: è¯„ä¼°å‚æ•°
        """
        self.train_ratio = kwargs.get('train_ratio', 0.8)
        self.random_state = kwargs.get('random_state', 42)
        self.classifiers = kwargs.get('classifiers', ['logistic'])
        self.metrics = kwargs.get('metrics', ['accuracy', 'f1_micro', 'f1_macro'])
        
        # åˆ†ç±»å™¨æ˜ å°„
        self.classifier_map = {
            'logistic': LogisticRegression(max_iter=1000, random_state=self.random_state),
            'svm': SVC(kernel='rbf', random_state=self.random_state),
            'rf': RandomForestClassifier(n_estimators=100, random_state=self.random_state)
        }
    
    def evaluate_single(self, embeddings: Dict[str, np.ndarray], 
                       X: List, Y: List, 
                       classifier: str = 'logistic') -> Dict[str, float]:
        """
        ä½¿ç”¨å•ä¸ªåˆ†ç±»å™¨è¯„ä¼°åµŒå…¥
        
        Args:
            embeddings: èŠ‚ç‚¹åµŒå…¥å­—å…¸
            X: èŠ‚ç‚¹åˆ—è¡¨
            Y: æ ‡ç­¾åˆ—è¡¨
            classifier: åˆ†ç±»å™¨åç§°
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if classifier not in self.classifier_map:
            raise ValueError(f"æœªçŸ¥åˆ†ç±»å™¨: {classifier}")
        
        clf_model = self.classifier_map[classifier]
        clf = Classifier(embeddings=embeddings, clf=clf_model)
        
        try:
            metrics = clf.split_train_evaluate(X, Y, self.train_ratio)
            return metrics
        except Exception as e:
            print(f"   âŒ è¯„ä¼°å¤±è´¥ ({classifier}): {e}")
            return {'acc': 0.0, 'micro': 0.0, 'macro': 0.0}
    
    def evaluate_multiple(self, embeddings: Dict[str, np.ndarray], 
                         X: List, Y: List) -> Dict[str, Dict[str, float]]:\n        \"\"\"\n        ä½¿ç”¨å¤šä¸ªåˆ†ç±»å™¨è¯„ä¼°åµŒå…¥\n        \n        Args:\n            embeddings: èŠ‚ç‚¹åµŒå…¥å­—å…¸\n            X: èŠ‚ç‚¹åˆ—è¡¨\n            Y: æ ‡ç­¾åˆ—è¡¨\n            \n        Returns:\n            {classifier_name: metrics} çš„å­—å…¸\n        \"\"\"\n        results = {}\n        \n        for classifier in self.classifiers:\n            print(f\"   ğŸ“Š è¯„ä¼° {classifier}...\")\n            metrics = self.evaluate_single(embeddings, X, Y, classifier)\n            results[classifier] = metrics\n            \n            if metrics['acc'] > 0:\n                print(f\"      å‡†ç¡®ç‡: {metrics['acc']:.4f}, F1-Micro: {metrics['micro']:.4f}\")\n        \n        return results\n    \n    def evaluate_algorithm(self, algorithm, X: List, Y: List, \n                          method_name: str = None) -> Dict[str, Any]:\n        \"\"\"\n        å®Œæ•´è¯„ä¼°å•ä¸ªç®—æ³•\n        \n        Args:\n            algorithm: ç®—æ³•å®ä¾‹\n            X: èŠ‚ç‚¹åˆ—è¡¨\n            Y: æ ‡ç­¾åˆ—è¡¨\n            method_name: æ–¹æ³•åç§°\n            \n        Returns:\n            å®Œæ•´è¯„ä¼°ç»“æœ\n        \"\"\"\n        if method_name is None:\n            method_name = getattr(algorithm, 'get_method_name', lambda: 'Unknown')()\n        \n        print(f\"\\nğŸ”¬ è¯„ä¼° {method_name}...\")\n        \n        # è®­ç»ƒè®¡æ—¶\n        start_time = time.time()\n        \n        try:\n            # è®­ç»ƒç®—æ³•\n            algorithm.train()\n            embeddings = algorithm.get_embeddings()\n            training_time = time.time() - start_time\n            \n            # è¯„ä¼°åµŒå…¥\n            classifier_results = self.evaluate_multiple(embeddings, X, Y)\n            \n            # æ±‡æ€»ç»“æœ\n            result = {\n                'method_name': method_name,\n                'training_time': training_time,\n                'embedding_size': len(embeddings),\n                'embedding_dim': list(embeddings.values())[0].shape[0] if embeddings else 0,\n                'classifier_results': classifier_results,\n                'success': True\n            }\n            \n            # è®¡ç®—æœ€ä½³æŒ‡æ ‡\n            best_metrics = self._get_best_metrics(classifier_results)\n            result.update(best_metrics)\n            \n            print(f\"   âœ… è®­ç»ƒæ—¶é—´: {training_time:.2f}s, æœ€ä½³å‡†ç¡®ç‡: {result['best_accuracy']:.4f}\")\n            \n        except Exception as e:\n            print(f\"   âŒ è¯„ä¼°å¤±è´¥: {e}\")\n            result = {\n                'method_name': method_name,\n                'training_time': 0,\n                'embedding_size': 0,\n                'embedding_dim': 0,\n                'classifier_results': {},\n                'success': False,\n                'error': str(e),\n                'best_accuracy': 0.0,\n                'best_f1_micro': 0.0,\n                'best_f1_macro': 0.0\n            }\n        \n        return result\n    \n    def _get_best_metrics(self, classifier_results: Dict[str, Dict[str, float]]) -> Dict[str, float]:\n        \"\"\"\n        ä»å¤šä¸ªåˆ†ç±»å™¨ç»“æœä¸­è·å–æœ€ä½³æŒ‡æ ‡\n        \n        Args:\n            classifier_results: åˆ†ç±»å™¨ç»“æœå­—å…¸\n            \n        Returns:\n            æœ€ä½³æŒ‡æ ‡å­—å…¸\n        \"\"\"\n        if not classifier_results:\n            return {'best_accuracy': 0.0, 'best_f1_micro': 0.0, 'best_f1_macro': 0.0}\n        \n        best_acc = max(results['acc'] for results in classifier_results.values())\n        best_micro = max(results['micro'] for results in classifier_results.values())\n        best_macro = max(results['macro'] for results in classifier_results.values())\n        \n        return {\n            'best_accuracy': best_acc,\n            'best_f1_micro': best_micro,\n            'best_f1_macro': best_macro\n        }\n    \n    def compare_algorithms(self, algorithms: List, X: List, Y: List) -> Dict[str, Any]:\n        \"\"\"\n        æ¯”è¾ƒå¤šä¸ªç®—æ³•\n        \n        Args:\n            algorithms: ç®—æ³•åˆ—è¡¨\n            X: èŠ‚ç‚¹åˆ—è¡¨\n            Y: æ ‡ç­¾åˆ—è¡¨\n            \n        Returns:\n            æ¯”è¾ƒç»“æœå­—å…¸\n        \"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ç®—æ³•æ¯”è¾ƒè¯„ä¼°\")\n        print(\"=\" * 80)\n        \n        results = {}\n        \n        for i, algorithm in enumerate(algorithms):\n            try:\n                method_name = getattr(algorithm, 'get_method_name', lambda: f'Method_{i+1}')()\n                result = self.evaluate_algorithm(algorithm, X, Y, method_name)\n                results[method_name] = result\n            except Exception as e:\n                print(f\"   âŒ ç®—æ³• {i+1} è¯„ä¼°å¤±è´¥: {e}\")\n                continue\n        \n        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š\n        comparison_report = self._generate_comparison_report(results)\n        \n        return {\n            'individual_results': results,\n            'comparison_report': comparison_report\n        }\n    \n    def _generate_comparison_report(self, results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š\n        \n        Args:\n            results: ç®—æ³•ç»“æœå­—å…¸\n            \n        Returns:\n            æ¯”è¾ƒæŠ¥å‘Š\n        \"\"\"\n        successful_results = {k: v for k, v in results.items() if v['success']}\n        \n        if not successful_results:\n            return {'error': 'æ²¡æœ‰æˆåŠŸçš„ç®—æ³•ç»“æœ'}\n        \n        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•\n        best_method = max(successful_results.keys(), \n                         key=lambda k: successful_results[k]['best_accuracy'])\n        best_accuracy = successful_results[best_method]['best_accuracy']\n        \n        # è®¡ç®—ç›¸å¯¹æ”¹è¿›\n        baseline_method = list(successful_results.keys())[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½œä¸ºåŸºå‡†\n        baseline_accuracy = successful_results[baseline_method]['best_accuracy']\n        \n        improvements = {}\n        for method, result in successful_results.items():\n            if method != baseline_method:\n                if baseline_accuracy > 0:\n                    improvement = (result['best_accuracy'] - baseline_accuracy) / baseline_accuracy * 100\n                else:\n                    improvement = 0\n                improvements[method] = improvement\n        \n        # æ•ˆç‡åˆ†æ\n        efficiency_analysis = {}\n        baseline_time = successful_results[baseline_method]['training_time']\n        for method, result in successful_results.items():\n            if result['training_time'] > 0:\n                speedup = baseline_time / result['training_time']\n            else:\n                speedup = 0\n            efficiency_analysis[method] = {\n                'training_time': result['training_time'],\n                'speedup': speedup\n            }\n        \n        return {\n            'total_methods': len(results),\n            'successful_methods': len(successful_results),\n            'best_method': best_method,\n            'best_accuracy': best_accuracy,\n            'baseline_method': baseline_method,\n            'baseline_accuracy': baseline_accuracy,\n            'improvements': improvements,\n            'efficiency_analysis': efficiency_analysis\n        }\n    \n    def print_comparison_report(self, comparison_data: Dict[str, Any]) -> None:\n        \"\"\"\n        æ‰“å°æ¯”è¾ƒæŠ¥å‘Š\n        \n        Args:\n            comparison_data: æ¯”è¾ƒæ•°æ®\n        \"\"\"\n        report = comparison_data['comparison_report']\n        results = comparison_data['individual_results']\n        \n        if 'error' in report:\n            print(f\"âŒ {report['error']}\")\n            return\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"ğŸ“Š ç®—æ³•æ¯”è¾ƒæŠ¥å‘Š\")\n        print(\"=\" * 80)\n        \n        # ç»“æœæ±‡æ€»è¡¨\n        print(f\"\\n{'æ–¹æ³•':<25} {'æˆåŠŸ':<6} {'å‡†ç¡®ç‡':<10} {'F1-Micro':<10} {'F1-Macro':<10} {'æ—¶é—´(s)':<10}\")\n        print(\"-\" * 80)\n        \n        for method, result in results.items():\n            status = \"âœ…\" if result['success'] else \"âŒ\"\n            accuracy = result.get('best_accuracy', 0)\n            f1_micro = result.get('best_f1_micro', 0)\n            f1_macro = result.get('best_f1_macro', 0)\n            time_taken = result.get('training_time', 0)\n            \n            print(f\"{method:<25} {status:<6} {accuracy:<10.4f} {f1_micro:<10.4f} {f1_macro:<10.4f} {time_taken:<10.2f}\")\n        \n        # æœ€ä½³æ–¹æ³•\n        print(f\"\\nğŸ† æœ€ä½³æ–¹æ³•: {report['best_method']}\")\n        print(f\"ğŸ† æœ€ä½³å‡†ç¡®ç‡: {report['best_accuracy']:.4f}\")\n        \n        # ç›¸å¯¹æ”¹è¿›\n        if report['improvements']:\n            print(f\"\\nğŸ“ˆ ç›¸å¯¹ {report['baseline_method']} çš„æ”¹è¿›:\")\n            for method, improvement in report['improvements'].items():\n                emoji = \"ğŸ“ˆ\" if improvement > 0 else \"ğŸ“‰\"\n                print(f\"   {emoji} {method}: {improvement:+.1f}%\")\n        \n        # æ•ˆç‡åˆ†æ\n        print(f\"\\nâš¡ æ•ˆç‡åˆ†æ:\")\n        for method, efficiency in report['efficiency_analysis'].items():\n            print(f\"   {method}: {efficiency['training_time']:.2f}s ({efficiency['speedup']:.1f}x)\")\n\nif __name__ == \"__main__\":\n    # æµ‹è¯•è¯„ä¼°å™¨\n    evaluator = Evaluator()\n    print(\"è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ\")\n    print(f\"æ”¯æŒçš„åˆ†ç±»å™¨: {list(evaluator.classifier_map.keys())}\")