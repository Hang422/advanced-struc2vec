#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„ graphlet å¢å¼º struc2vec è·ç¦»è®¡ç®—æ¨¡å—
ä¸»è¦æ”¹è¿›ï¼š
1. æ›´å¥½çš„ GDV é¢„å¤„ç†ç­–ç•¥
2. å¤šç§è·ç¦»åº¦é‡æ–¹æ³•
3. ç‰¹å¾é€‰æ‹©å’Œè‡ªé€‚åº”æƒé‡
4. æ›´çµæ´»çš„èåˆç­–ç•¥
"""
import os
import sys
import json
import pickle
import subprocess
import numpy as np
import networkx as nx
from collections import deque
from typing import Dict, Tuple, List, Optional
from sklearn.preprocessing import StandardScaler
from scipy.stats import entropy
from scipy.spatial.distance import cosine, euclidean


def _find_orca_executable():
    # finalVersion ç›®å½•æ˜¯å½“å‰é¡¹ç›®æ ¹ç›®å½•
    finalVersion_root = os.path.abspath(os.path.join(__file__, os.pardir, os.pardir, os.pardir, os.pardir))
    # çœŸæ­£çš„é¡¹ç›®æ ¹ç›®å½•åœ¨ä¸Šä¸€çº§
    project_root = os.path.dirname(finalVersion_root)
    
    for name in ("orca", "orca.exe"):
        # å…ˆå°è¯•é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ libs/orca/ ç›®å½•
        p = os.path.join(project_root, "libs", "orca", name)
        if os.path.isfile(p) and os.access(p, os.X_OK):
            return p
        
        # å¤‡ç”¨ï¼šfinalVersion å†…éƒ¨ libs/orca/ ç›®å½•
        p = os.path.join(finalVersion_root, "libs", "orca", name)
        if os.path.isfile(p) and os.access(p, os.X_OK):
            return p
    
    from shutil import which
    for name in ("orca", "orca.exe"):
        p = which(name)
        if p:
            return p
    
    raise FileNotFoundError("Cannot find orca binary")


def preprocess_edgelist(input_path: str, output_path: str) -> tuple[dict, dict]:
    """å°†åŸå§‹ .edgelist æ–‡ä»¶è½¬æ¢ä¸º ORCA å¯è¯»æ ¼å¼"""
    G = nx.read_edgelist(input_path, nodetype=int)
    mapping = {node: i for i, node in enumerate(sorted(G.nodes()))}
    reverse = {i: node for node, i in mapping.items()}
    H = nx.relabel_nodes(G, mapping)
    H.remove_edges_from(nx.selfloop_edges(H))
    n = H.number_of_nodes()
    e = H.number_of_edges()
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"{n} {e}\n")
        for u, v in H.edges():
            f.write(f"{u} {v}\n")
    return mapping, reverse


def compute_node_gdv(edgelist_path: str, k: int = 5, orca_bin: str = None, mode: str = 'node') -> dict[int, np.ndarray]:
    """è°ƒç”¨ ORCA è®¡ç®—èŠ‚ç‚¹çš„ graphlet åº¦å‘é‡"""
    if orca_bin is None:
        orca_bin = _find_orca_executable()
    out_path = f'{edgelist_path}_GDV.out'
    cmd = [orca_bin, mode, str(k), edgelist_path, out_path]
    subprocess.run(cmd, check=True)
    gdv = {}
    with open(out_path, 'r') as f:
        for idx, line in enumerate(f):
            gdv[idx] = np.fromstring(line, dtype=int, sep=' ')
    return gdv


class ImprovedGDVPreprocessor:
    """æ”¹è¿›çš„ GDV é¢„å¤„ç†å™¨"""
    def __init__(self, normalization_method: str = 'adaptive'):
        self.normalization_method = normalization_method
        self.scaler = StandardScaler()
        self.orbit_importance = None
        
    def fit_transform(self, gdv: np.ndarray) -> np.ndarray:
        """
        å¯¹ GDV è¿›è¡Œæ™ºèƒ½é¢„å¤„ç†
        - adaptive: æ ¹æ® orbit åˆ†å¸ƒè‡ªé€‚åº”é€‰æ‹©å½’ä¸€åŒ–æ–¹æ³•
        - zscore: æ ‡å‡†åŒ–
        - log: log å˜æ¢
        - sqrt: å¹³æ–¹æ ¹å˜æ¢
        """
        n_nodes, n_orbits = gdv.shape
        processed_gdv = np.zeros_like(gdv, dtype=float)
        
        if self.normalization_method == 'adaptive':
            # å¯¹æ¯ä¸ª orbit åˆ†åˆ«å¤„ç†
            for i in range(n_orbits):
                orbit_values = gdv[:, i]
                
                # è®¡ç®—åˆ†å¸ƒç‰¹æ€§
                sparsity = np.mean(orbit_values == 0)
                if sparsity > 0.9:  # éå¸¸ç¨€ç–çš„ orbit
                    # ä½¿ç”¨äºŒå€¼åŒ–
                    processed_gdv[:, i] = (orbit_values > 0).astype(float)
                elif np.max(orbit_values) > 100:  # å€¼åŸŸå¾ˆå¤§çš„ orbit
                    # ä½¿ç”¨ log å˜æ¢
                    processed_gdv[:, i] = np.log1p(orbit_values)
                else:
                    # ä½¿ç”¨ z-score æ ‡å‡†åŒ–
                    if np.std(orbit_values) > 0:
                        processed_gdv[:, i] = (orbit_values - np.mean(orbit_values)) / np.std(orbit_values)
                    else:
                        processed_gdv[:, i] = 0
                        
        elif self.normalization_method == 'zscore':
            processed_gdv = self.scaler.fit_transform(gdv)
            
        elif self.normalization_method == 'log':
            processed_gdv = np.log1p(gdv)
            
        elif self.normalization_method == 'sqrt':
            processed_gdv = np.sqrt(gdv)
            
        # è®¡ç®— orbit é‡è¦æ€§ï¼ˆåŸºäºæ–¹å·®ï¼‰
        self.orbit_importance = np.var(processed_gdv, axis=0)
        self.orbit_importance = self.orbit_importance / np.sum(self.orbit_importance)
        
        return processed_gdv
    
    def select_important_orbits(self, gdv: np.ndarray, top_k: int = 40) -> Tuple[np.ndarray, List[int]]:
        """é€‰æ‹©æœ€é‡è¦çš„ k ä¸ª orbits"""
        if self.orbit_importance is None:
            raise ValueError("éœ€è¦å…ˆè°ƒç”¨ fit_transform")
            
        important_indices = np.argsort(self.orbit_importance)[-top_k:]
        return gdv[:, important_indices], important_indices


def enhanced_graphlet_correlation(M: np.ndarray, weights: Optional[np.ndarray] = None) -> np.ndarray:
    """å¢å¼ºçš„ç›¸å…³æ€§çŸ©é˜µè®¡ç®—ï¼Œæ”¯æŒåŠ æƒ"""
    n, z = M.shape
    
    if weights is not None:
        # åŠ æƒå¹³å‡
        weighted_mean = np.average(M, axis=0, weights=weights)
        X = M - weighted_mean
        # åŠ æƒåæ–¹å·®
        cov = np.zeros((z, z))
        for i in range(n):
            cov += weights[i] * np.outer(X[i], X[i])
        cov /= np.sum(weights)
    else:
        X = M - M.mean(axis=0, keepdims=True)
        cov = (X.T @ X) / (n - 1) if n > 1 else np.zeros((z, z), dtype=float)
    
    diag = np.diag(cov)
    std = np.sqrt(np.maximum(diag, 0))
    std_safe = std.copy()
    std_safe[std_safe == 0] = 1.0
    
    C = cov / std_safe[:, None] / std_safe[None, :]
    zero_std = (std == 0)
    C[zero_std, :] = 0.0
    C[:, zero_std] = 0.0
    
    return C


class MultiMetricDistance:
    """å¤šåº¦é‡è·ç¦»è®¡ç®—å™¨"""
    
    @staticmethod
    def matrix_distance(C1: np.ndarray, C2: np.ndarray, method: str = 'combined') -> float:
        """
        è®¡ç®—ä¸¤ä¸ªç›¸å…³æ€§çŸ©é˜µä¹‹é—´çš„è·ç¦»
        method: 'frobenius', 'eigenvalue', 'trace', 'combined'
        """
        if method == 'frobenius':
            return np.linalg.norm(C1 - C2, 'fro')
            
        elif method == 'eigenvalue':
            # ä½¿ç”¨å‰ k ä¸ªç‰¹å¾å€¼çš„è·ç¦»
            k = min(5, C1.shape[0])
            try:
                eigvals1 = np.sort(np.linalg.eigvalsh(C1))[-k:]
                eigvals2 = np.sort(np.linalg.eigvalsh(C2))[-k:]
                return np.linalg.norm(eigvals1 - eigvals2)
            except:
                return np.linalg.norm(C1 - C2, 'fro')
                
        elif method == 'trace':
            return abs(np.trace(C1) - np.trace(C2))
            
        elif method == 'combined':
            # ç»„åˆå¤šç§è·ç¦»åº¦é‡
            distances = []
            
            # Frobenius èŒƒæ•°
            distances.append(np.linalg.norm(C1 - C2, 'fro'))
            
            # è¿¹å·®å¼‚
            distances.append(abs(np.trace(C1) - np.trace(C2)))
            
            # ç‰¹å¾å€¼è·ç¦»ï¼ˆå¦‚æœçŸ©é˜µä¸å¤ªå°ï¼‰
            if C1.shape[0] >= 3:
                try:
                    k = min(3, C1.shape[0])
                    eigvals1 = np.sort(np.linalg.eigvalsh(C1))[-k:]
                    eigvals2 = np.sort(np.linalg.eigvalsh(C2))[-k:]
                    distances.append(np.linalg.norm(eigvals1 - eigvals2))
                except:
                    distances.append(0)
            
            # åŠ æƒç»„åˆ
            weights = [0.5, 0.3, 0.2] if len(distances) == 3 else [0.7, 0.3]
            return np.sum([w * d for w, d in zip(weights, distances)])
    
    @staticmethod
    def vector_distance(v1: np.ndarray, v2: np.ndarray, method: str = 'weighted_l1') -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è·ç¦»"""
        if method == 'l1':
            return np.linalg.norm(v1 - v2, ord=1)
        elif method == 'l2':
            return np.linalg.norm(v1 - v2, ord=2)
        elif method == 'cosine':
            return cosine(v1, v2) if np.any(v1) and np.any(v2) else 0
        elif method == 'weighted_l1':
            # å¯¹ä¸åŒç»´åº¦ä½¿ç”¨ä¸åŒæƒé‡
            weights = 1 / (1 + np.arange(len(v1)))
            return np.sum(weights * np.abs(v1 - v2))


def compute_graphlet_distance_improved(
    graph: nx.Graph,
    node_gdv: np.ndarray,
    max_layer: int = 5,
    distance_method: str = 'combined',
    use_orbit_selection: bool = True,
    top_k_orbits: int = 40
) -> Dict[Tuple[int, int], Dict[int, float]]:
    """
    æ”¹è¿›çš„ graphlet è·ç¦»è®¡ç®—
    - æ”¯æŒå¤šç§è·ç¦»åº¦é‡
    - æ”¯æŒ orbit é€‰æ‹©
    - æ›´çµæ´»çš„å±‚æ¬¡èšåˆ
    """
    # é¢„å¤„ç† GDV
    preprocessor = ImprovedGDVPreprocessor('adaptive')
    processed_gdv = preprocessor.fit_transform(node_gdv)
    
    # é€‰æ‹©é‡è¦çš„ orbits
    if use_orbit_selection:
        processed_gdv, selected_orbits = preprocessor.select_important_orbits(
            processed_gdv, top_k_orbits
        )
    
    nodes = sorted(graph.nodes())
    n = len(nodes)
    idx = {v: i for i, v in enumerate(nodes)}
    
    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹æ¯å±‚çš„ç‰¹å¾
    layer_features = [{} for _ in range(n)]
    metric_calc = MultiMetricDistance()
    
    for ui, u in enumerate(nodes):
        visited = {u}
        q = deque([(u, 0)])
        layers = {0: [u]}
        
        # BFS è·å–æ¯å±‚èŠ‚ç‚¹
        while q:
            v, d = q.popleft()
            if d >= max_layer:
                continue
            for w in graph[v]:
                if w not in visited:
                    visited.add(w)
                    q.append((w, d + 1))
                    layers.setdefault(d + 1, []).append(w)
        
        # è®¡ç®—æ¯å±‚çš„ç‰¹å¾
        for â„“ in range(max_layer + 1):
            members = layers.get(â„“, [])
            if len(members) == 0:
                layer_features[ui][â„“] = {
                    'corr_matrix': np.zeros((processed_gdv.shape[1], processed_gdv.shape[1])),
                    'mean_vector': np.zeros(processed_gdv.shape[1]),
                    'std_vector': np.zeros(processed_gdv.shape[1]),
                    'size': 0
                }
            else:
                M = processed_gdv[[idx[w] for w in members], :]
                
                # è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§æƒé‡ï¼ˆåŸºäºåº¦ï¼‰
                degrees = np.array([graph.degree[w] for w in members])
                weights = degrees / np.sum(degrees) if np.sum(degrees) > 0 else None
                
                layer_features[ui][â„“] = {
                    'corr_matrix': enhanced_graphlet_correlation(M, weights),
                    'mean_vector': np.average(M, axis=0, weights=weights) if weights is not None else np.mean(M, axis=0),
                    'std_vector': np.std(M, axis=0),
                    'size': len(members)
                }
    
    # è®¡ç®—ä¸¤ä¸¤èŠ‚ç‚¹çš„è·ç¦»
    distances = {}
    for i in range(n):
        for j in range(i + 1, n):
            layer_distances = {}
            
            for â„“ in range(max_layer + 1):
                feat_i = layer_features[i][â„“]
                feat_j = layer_features[j][â„“]
                
                # ç»„åˆå¤šç§è·ç¦»
                if distance_method == 'combined':
                    # ç›¸å…³æ€§çŸ©é˜µè·ç¦»
                    corr_dist = metric_calc.matrix_distance(
                        feat_i['corr_matrix'], 
                        feat_j['corr_matrix'], 
                        'combined'
                    )
                    
                    # å‡å€¼å‘é‡è·ç¦»
                    mean_dist = metric_calc.vector_distance(
                        feat_i['mean_vector'],
                        feat_j['mean_vector'],
                        'weighted_l1'
                    )
                    
                    # æ ‡å‡†å·®å‘é‡è·ç¦»
                    std_dist = metric_calc.vector_distance(
                        feat_i['std_vector'],
                        feat_j['std_vector'],
                        'l2'
                    )
                    
                    # å¤§å°å·®å¼‚
                    size_diff = abs(feat_i['size'] - feat_j['size']) / max(feat_i['size'], feat_j['size'], 1)
                    
                    # åŠ æƒç»„åˆï¼Œè¿‘å±‚æƒé‡æ›´é«˜
                    layer_weight = np.exp(-â„“ / 2)
                    layer_distances[â„“] = layer_weight * (
                        0.4 * corr_dist + 
                        0.3 * mean_dist + 
                        0.2 * std_dist + 
                        0.1 * size_diff
                    )
                else:
                    # ä½¿ç”¨å•ä¸€è·ç¦»åº¦é‡
                    layer_distances[â„“] = metric_calc.matrix_distance(
                        feat_i['corr_matrix'],
                        feat_j['corr_matrix'],
                        distance_method
                    )
            
            # å±‚æ¬¡èšåˆï¼ˆä½¿ç”¨ç´¯ç§¯å’Œï¼Œä½†ä¿ç•™åŸå§‹å€¼ï¼‰
            cum_distances = {}
            cum = 0.0
            for â„“ in range(max_layer + 1):
                if â„“ in layer_distances and layer_distances[â„“] is not None:
                    cum += layer_distances[â„“]
                cum_distances[â„“] = cum
                
            distances[(nodes[i], nodes[j])] = cum_distances
    
    return distances


def adaptive_fusion(
    graphlet_dist: Dict,
    degree_dist: Dict,
    graph: nx.Graph
) -> Dict:
    """
    è‡ªé€‚åº”èåˆ graphlet å’Œåº¦åºåˆ—è·ç¦»
    æ ¹æ®èŠ‚ç‚¹ç‰¹æ€§åŠ¨æ€è°ƒæ•´æƒé‡
    """
    fused = {}
    
    # è®¡ç®—å›¾çš„å…¨å±€ç‰¹æ€§
    clustering = nx.average_clustering(graph)
    density = nx.density(graph)
    
    for pair in set(graphlet_dist.keys()).intersection(degree_dist.keys()):
        u, v = pair
        
        # æ ¹æ®èŠ‚ç‚¹çš„å±€éƒ¨ç‰¹æ€§è°ƒæ•´æƒé‡
        u_clustering = nx.clustering(graph, u)
        v_clustering = nx.clustering(graph, v)
        avg_clustering = (u_clustering + v_clustering) / 2
        
        # é«˜èšç±»ç³»æ•°çš„èŠ‚ç‚¹å¯¹ï¼Œgraphlet æƒé‡æ›´é«˜
        if avg_clustering > clustering:
            alpha = 0.7  # graphlet æƒé‡
        else:
            alpha = 0.5
        
        layers_g = graphlet_dist[pair]
        layers_d = degree_dist[pair]
        fused[pair] = {}
        
        for l in set(layers_g.keys()).intersection(layers_d.keys()):
            fused[pair][l] = alpha * layers_g[l] + (1 - alpha) * layers_d[l]
    
    return fused


def generate_improved_structural_distance(
    edgelist_path: str,
    output_path: str,
    k: int = 5,
    max_layer: int = 5,
    distance_method: str = 'combined',
    use_orbit_selection: bool = True,
    top_k_orbits: int = 40
):
    """ç”Ÿæˆæ”¹è¿›çš„ç»“æ„è·ç¦»æ–‡ä»¶"""
    print(f"ğŸ“‚ åŠ è½½å›¾ï¼š{edgelist_path}")
    
    # å‡†å¤‡ ORCA è¾“å…¥
    orca_input = f"{edgelist_path}.in"
    mapping, reverse = preprocess_edgelist(edgelist_path, orca_input)
    
    print("ğŸš€ è°ƒç”¨ ORCA è®¡ç®—èŠ‚ç‚¹ GDV ...")
    gdv_dict = compute_node_gdv(orca_input, k=k)
    node_gdv = np.array([gdv_dict[i] for i in range(len(mapping))])
    
    # åŠ è½½å›¾
    G = nx.read_edgelist(edgelist_path, nodetype=int)
    G = nx.relabel_nodes(G, mapping)
    
    print("ğŸ§  è®¡ç®—æ”¹è¿›çš„ç»“æ„è·ç¦»...")
    raw_dist = compute_graphlet_distance_improved(
        G, node_gdv, 
        max_layer=max_layer,
        distance_method=distance_method,
        use_orbit_selection=use_orbit_selection,
        top_k_orbits=top_k_orbits
    )
    
    print("ğŸ” æ¢å¤åŸå§‹èŠ‚ç‚¹ç¼–å·...")
    final_dist = {}
    for (u, v), layer_d in raw_dist.items():
        final_dist[(reverse[u], reverse[v])] = layer_d
    
    print(f"ğŸ’¾ ä¿å­˜åˆ°ï¼š{output_path}")
    with open(output_path, "wb") as f:
        pickle.dump(final_dist, f)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(orca_input)
    print("âœ… å®Œæˆï¼")


if __name__ == '__main__':
    # æµ‹è¯•æ”¹è¿›çš„æ–¹æ³•
    generate_improved_structural_distance(
        "../data/flight/brazil-airports.edgelist",
        "output/structural_dist_improved.pkl",
        max_layer=5,
        distance_method='combined',
        use_orbit_selection=True,
        top_k_orbits=40
    )